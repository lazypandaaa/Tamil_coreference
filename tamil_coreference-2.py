# -*- coding: utf-8 -*-
"""Tamil-coreference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cGdWL3H1XosOdRHAxMNSzGvd_wnTNcsp
"""

import os
import json
import re
from typing import List, Dict, Tuple
from pydantic import BaseModel
import matplotlib.pyplot as plt
import networkx as nx
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from google import genai
import warnings
warnings.filterwarnings('ignore')

# Set up your API key
# Replace 'YOUR_API_KEY' with your actual Gemini API key

GEMINI_API_KEY = "AIzaSyD4dxXY78p58XjOaloTW0r6bLq92eQOOkc"  # Get this from Google AI Studio
GEMINI_API_KEY = "AIzaSyARKBMZglTBtSNQTbPIac6IErs_63yaREQ"
os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY

# For Colab, you can use the secrets feature or set it directly
API_KEY = "AIzaSyD4dxXY78p58XjOaloTW0r6bLq92eQOOkc"  # Replace with your actual API key
os.environ['GOOGLE_API_KEY'] = API_KEY

# Initialize the Gemini client
client = genai.Client(api_key=API_KEY)

print("âœ… Libraries imported and API client initialized")

# Cell 2: Define Pydantic models for structured output
class Mention(BaseModel):
    text: str
    start_index: int
    end_index: int
    sentence_index: int

class CoreferenceCluster(BaseModel):
    cluster_id: int
    entity_name: str
    mentions: List[Mention]
    entity_type: str  # PERSON, PLACE, ORGANIZATION, etc.

class CoreferenceResult(BaseModel):
    text: str
    clusters: List[CoreferenceCluster]
    total_mentions: int

print("Data models defined successfully!")

# Cell 3: Tamil text processing and word-to-number mapping
class TamilTextProcessor:
    def __init__(self):
        self.word_to_number = {}
        self.number_to_word = {}
        self.word_counter = 1

    def split_sentences(self, text: str) -> List[str]:
        """Split Tamil text into sentences"""
        # Tamil sentence delimiters
        sentences = re.split(r'[à¥¤|.|\?|\!]', text)
        return [s.strip() for s in sentences if s.strip()]

    def tokenize_tamil(self, text: str) -> List[str]:
        """Simple Tamil tokenization"""
        # Split by spaces and common punctuation
        tokens = re.findall(r'\S+', text)
        return tokens

    def map_words_to_numbers(self, text: str) -> Dict[str, int]:
        """Map each unique Tamil word to a number"""
        words = self.tokenize_tamil(text)
        for word in words:
            if word not in self.word_to_number:
                self.word_to_number[word] = self.word_counter
                self.number_to_word[self.word_counter] = word
                self.word_counter += 1
        return self.word_to_number

    def get_word_positions(self, text: str) -> List[Tuple[str, int, int]]:
        """Get word positions in text"""
        words = []
        position = 0
        for word in self.tokenize_tamil(text):
            start = text.find(word, position)
            end = start + len(word)
            words.append((word, start, end))
            position = end
        return words

# Initialize processor
processor = TamilTextProcessor()
print("Tamil text processor initialized!")

# Cell 4: Gemini API based coreference resolution
class GeminiCoreferenceResolver:
    def __init__(self, api_key: str):
        self.client = genai.Client(api_key=api_key)

    def resolve_coreferences(self, tamil_text: str) -> CoreferenceResult:
        """Use Gemini to identify coreferences in Tamil text"""

        prompt = f"""
        Analyze the following Tamil text and identify coreference clusters. A coreference cluster contains all mentions that refer to the same entity.

        Tamil Text: "{tamil_text}"

        Instructions:
        1. Identify all mentions (nouns, pronouns, proper names) that refer to entities
        2. Group mentions that refer to the same entity into clusters
        3. For each mention, provide:
           - The exact text of the mention
           - Start and end character positions in the original text
           - Which sentence it appears in (0-indexed)
        4. Classify entity types as: PERSON, PLACE, ORGANIZATION, or OTHER
        5. Give each cluster a meaningful entity name

        Example format expected:
        - If "à®°à®¾à®®à¯" appears at position 0-3 and "à®…à®µà®©à¯" at position 20-24 both refer to the same person, they should be in the same cluster.

        Be precise with character positions and ensure all referring expressions are captured.
        """

        try:
            response = self.client.models.generate_content(
            model="models/gemini-2.5-flash",
            contents=prompt,
            config={
                "response_mime_type": "application/json",
                "response_schema": CoreferenceResult,
            }
            )
              

            result = json.loads(response.text)
            return CoreferenceResult(**result)

        except Exception as e:
            print(f"Error with  API: {e}")
            # Return empty result
            return CoreferenceResult(
                text=tamil_text,
                clusters=[],
                total_mentions=0
            )

# Initialize resolver (replace with your actual API key)
try:
    resolver = GeminiCoreferenceResolver(GEMINI_API_KEY)
    print("Coreference Resolver initialized!")
except Exception as e:
    print(f"Error initializing resolver: {e}")
    print("Please make sure you've set your API_KEY correctly")

# Cell 5: Visualization functions
class CoreferenceVisualizer:
    def __init__(self, processor: TamilTextProcessor):
        self.processor = processor
        self.colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']

    def create_word_mapping_display(self, text: str) -> None:
        """Display word-to-number mapping"""
        word_mapping = self.processor.map_words_to_numbers(text)

        print("=" * 50)
        print("WORD TO NUMBER MAPPING")
        print("=" * 50)

        for word, number in word_mapping.items():
            print(f"{number:2d} -> {word}")

        print("\nMapped Text (using numbers):")
        words = self.processor.tokenize_tamil(text)
        mapped_text = " ".join([str(word_mapping[word]) for word in words])
        print(mapped_text)

    def create_cluster_visualization(self, result: CoreferenceResult) -> None:
        """Create a simple text-based cluster visualization"""
        print("\n" + "=" * 50)
        print("COREFERENCE CLUSTERS DETECTED")
        print("=" * 50)

        for i, cluster in enumerate(result.clusters):
            color = self.colors[i % len(self.colors)]
            print(f"\nğŸ”— Cluster {cluster.cluster_id}: {cluster.entity_name} ({cluster.entity_type})")
            print(f"   Color: {color.upper()}")

            mentions_text = []
            mentions_numbers = []

            for mention in cluster.mentions:
                word_num = self.processor.word_to_number.get(mention.text, 0)
                mentions_text.append(mention.text)
                mentions_numbers.append(str(word_num))

            print(f"   Tamil: {' â†’ '.join(mentions_text)}")
            print(f"   Numbers: {' â†’ '.join(mentions_numbers)}")

    def create_arc_visualization(self, result: CoreferenceResult) -> None:
        """Create arc-based visualization using matplotlib"""
        if not result.clusters:
            print("No coreference clusters found to visualize.")
            return

        # Prepare data for visualization
        sentences = self.processor.split_sentences(result.text)
        all_words = []
        word_positions = []

        pos = 0
        for sent_idx, sentence in enumerate(sentences):
            words = self.processor.tokenize_tamil(sentence)
            for word in words:
                all_words.append(word)
                word_positions.append((word, pos, sent_idx))
                pos += 1

        # Create the plot
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))

        # Top plot: Words with numbers
        word_numbers = [self.processor.word_to_number.get(word, 0) for word in all_words]
        ax1.bar(range(len(all_words)), word_numbers, alpha=0.7)
        ax1.set_title('Words as Numbers')
        ax1.set_xlabel('Word Position')
        ax1.set_ylabel('Word Number')
        ax1.grid(True, alpha=0.3)

        # Bottom plot: Arc visualization
        ax2.set_xlim(-0.5, len(all_words) - 0.5)
        ax2.set_ylim(-2, 3)

        # Draw words
        for i, word in enumerate(all_words):
            word_num = self.processor.word_to_number.get(word, 0)
            ax2.text(i, -1, f"{word_num}", ha='center', va='center',
                    bbox=dict(boxstyle="round,pad=0.3", facecolor='lightblue'))

        # Draw arcs for each cluster
        for cluster_idx, cluster in enumerate(result.clusters):
            color = self.colors[cluster_idx % len(self.colors)]
            mention_positions = []

            # Find positions of mentions
            for mention in cluster.mentions:
                for pos, word in enumerate(all_words):
                    if word == mention.text:
                        mention_positions.append(pos)

            # Draw arcs between mentions
            for i in range(len(mention_positions) - 1):
                start_pos = mention_positions[i]
                end_pos = mention_positions[i + 1]

                # Calculate arc
                mid_x = (start_pos + end_pos) / 2
                height = 0.5 + cluster_idx * 0.3

                # Draw arc
                arc_x = np.linspace(start_pos, end_pos, 100)
                arc_y = height * (1 - 4 * (arc_x - mid_x) ** 2 / (end_pos - start_pos) ** 2)

                ax2.plot(arc_x, arc_y, color=color, linewidth=2,
                        label=f'Cluster {cluster.cluster_id}: {cluster.entity_name}' if i == 0 else "")

                # Add arrows
                ax2.annotate('', xy=(end_pos, 0.1), xytext=(start_pos, 0.1),
                           arrowprops=dict(arrowstyle='->', color=color, lw=1.5))

        ax2.set_title('Coreference Arc Visualization')
        ax2.set_xlabel('Word Position')
        ax2.set_yticks([])
        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

        plt.tight_layout()
        plt.show()

    def create_interactive_visualization(self, result: CoreferenceResult) -> None:
        """Create interactive visualization using Plotly"""
        if not result.clusters:
            print("No coreference clusters found to visualize.")
            return

        # Prepare data
        words = self.processor.tokenize_tamil(result.text)
        word_numbers = [self.processor.word_to_number.get(word, 0) for word in words]

        # Create subplots
        fig = make_subplots(
            rows=2, cols=1,
            subplot_titles=('Word Numbers', 'Coreference Arcs'),
            row_heights=[0.4, 0.6]
        )

        # Add bar chart
        fig.add_trace(
            go.Bar(x=list(range(len(words))), y=word_numbers,
                  name='Word Numbers', showlegend=False),
            row=1, col=1
        )

        # Add arc visualization
        for cluster_idx, cluster in enumerate(result.clusters):
            color = self.colors[cluster_idx % len(self.colors)]
            mention_positions = []

            for mention in cluster.mentions:
                for pos, word in enumerate(words):
                    if word == mention.text:
                        mention_positions.append(pos)

            # Draw arcs
            for i in range(len(mention_positions) - 1):
                start_pos = mention_positions[i]
                end_pos = mention_positions[i + 1]

                mid_x = (start_pos + end_pos) / 2
                height = 0.5 + cluster_idx * 0.3

                arc_x = np.linspace(start_pos, end_pos, 50)
                arc_y = [height * (1 - 4 * (x - mid_x) ** 2 / (end_pos - start_pos) ** 2) for x in arc_x]

                fig.add_trace(
                    go.Scatter(x=arc_x, y=arc_y, mode='lines',
                             name=f'Cluster {cluster.cluster_id}: {cluster.entity_name}',
                             line=dict(color=color, width=3)),
                    row=2, col=1
                )

        fig.update_layout(height=700, title_text="Tamil Coreference Resolution Visualization")
        fig.show()

# Initialize visualizer
visualizer = CoreferenceVisualizer(processor)
print("Visualization functions created!")

# Cell 6: Main analysis function
def analyze_tamil_coreference(tamil_text: str, show_visualizations: bool = True):
    """Complete coreference analysis pipeline"""

    print("ğŸ” TAMIL COREFERENCE RESOLUTION ANALYSIS")
    print("=" * 60)
    print(f"Input Text: {tamil_text}")
    print("\nğŸ“Š Processing...")

    # Step 1: Create word mapping
    if show_visualizations:
        visualizer.create_word_mapping_display(tamil_text)

    # Step 2: Resolve coreferences using Gemini
    print("\nğŸ¤– Analyzing coreferences ...")
    try:
        result = resolver.resolve_coreferences(tamil_text)

        if result.clusters:
            print(f"âœ… Found {len(result.clusters)} coreference clusters!")

            # Step 3: Display results
            if show_visualizations:
                visualizer.create_cluster_visualization(result)

                print("\nğŸ“ˆ Creating visualizations...")
                visualizer.create_arc_visualization(result)
                visualizer.create_interactive_visualization(result)
        else:
            print("âŒ No coreference clusters detected.")

        return result

    except Exception as e:
        print(f"âŒ Error during analysis: {e}")
        return None

print("Main analysis function ready!")

sample_tamil_text = "à®°à®¾à®®à¯ à®’à®°à¯ à®®à®¾à®£à®µà®©à¯. à®…à®µà®©à¯ à®¤à®©à®¤à¯ à®µà¯€à®Ÿà¯à®Ÿà¯ˆ à®µà®¿à®Ÿà¯à®Ÿà¯ à®•à®¿à®³à®®à¯à®ªà®¿à®©à®¾à®©à¯. à®šà¯€à®¤à®¾ à®’à®°à¯ à®†à®šà®¿à®°à®¿à®¯à¯ˆ. à®…à®µà®³à¯ à®…à®µà®©à¯ à®¤à®©à¯à®©à¯ˆ à®•à®£à¯à®Ÿ à®ªà¯‹à®¤à¯."

print("ğŸš€ Testing Tamil Coreference Resolution System")
print("=" * 50)

# Run the complete analysis
result = analyze_tamil_coreference(sample_tamil_text)

def interactive_demo():
    """Interactive demo for custom Tamil text input"""

    print("ğŸ¯ INTERACTIVE TAMIL COREFERENCE DEMO")
    print("=" * 50)
    print("Enter your Tamil text below, or press Enter to use default example:")
    print("Default: à®°à®¾à®®à¯ à®’à®°à¯ à®®à®¾à®£à®µà®©à¯. à®…à®µà®©à¯ à®¤à®©à®¤à¯ à®µà¯€à®Ÿà¯à®Ÿà¯ˆ à®µà®¿à®Ÿà¯à®Ÿà¯ à®•à®¿à®³à®®à¯à®ªà®¿à®©à®¾à®©à¯.")

    user_input = input("\nTamil Text: ").strip()

    if not user_input:
        user_input = "à®°à®¾à®®à¯ à®’à®°à¯ à®®à®¾à®£à®µà®©à¯. à®…à®µà®©à¯ à®¤à®©à®¤à¯ à®µà¯€à®Ÿà¯à®Ÿà¯ˆ à®µà®¿à®Ÿà¯à®Ÿà¯ à®•à®¿à®³à®®à¯à®ªà®¿à®©à®¾à®©à¯. à®šà¯€à®¤à®¾ à®’à®°à¯ à®†à®šà®¿à®°à®¿à®¯à¯ˆ. à®…à®µà®³à¯ à®…à®µà®©à¯ à®¤à®©à¯à®©à¯ˆ à®•à®£à¯à®Ÿ à®ªà¯‹à®¤à¯."
        print(f"Using default: {user_input}")

    return analyze_tamil_coreference(user_input)

# Utility function to save results
def save_results_to_file(result: CoreferenceResult, filename: str = "tamil_coreference_results.json"):
    """Save analysis results to JSON file"""
    if result:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result.dict(), f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ Results saved to {filename}")
    else:
        print("âŒ No results to save")

# Function to load and analyze from file
def analyze_from_file(filepath: str):
    """Load Tamil text from file and analyze"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            text = f.read().strip()

        print(f"ğŸ“– Loaded text from {filepath}")
        return analyze_tamil_coreference(text)

    except FileNotFoundError:
        print(f"âŒ File {filepath} not found")
        return None
    except Exception as e:
        print(f"âŒ Error reading file: {e}")
        return None

def create_network_visualization(result: CoreferenceResult):
    """Create a network graph showing coreference relationships"""

    if not result.clusters:
        print("No clusters to visualize in network format.")
        return

    # Create network graph
    G = nx.Graph()

    # Add nodes and edges for each cluster
    for cluster in result.clusters:
        mentions = [mention.text for mention in cluster.mentions]
        mention_numbers = [str(processor.word_to_number.get(mention.text, 0)) for mention in cluster.mentions]

        # Add nodes
        for i, mention in enumerate(mentions):
            node_id = f"{mention}_{cluster.cluster_id}_{i}"
            G.add_node(node_id,
                      label=f"{mention}\n({mention_numbers[i]})",
                      cluster=cluster.cluster_id,
                      entity_type=cluster.entity_type)

        # Add edges within cluster
        cluster_nodes = [f"{mention}_{cluster.cluster_id}_{i}" for i, mention in enumerate(mentions)]
        for i in range(len(cluster_nodes)):
            for j in range(i + 1, len(cluster_nodes)):
                G.add_edge(cluster_nodes[i], cluster_nodes[j], cluster_id=cluster.cluster_id)

    # Create layout
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G, k=2, iterations=50)

    # Draw nodes by cluster
    for cluster_idx, cluster in enumerate(result.clusters):
        cluster_nodes = [node for node in G.nodes() if G.nodes[node]['cluster'] == cluster.cluster_id]
        color = visualizer.colors[cluster_idx % len(visualizer.colors)]

        nx.draw_networkx_nodes(G, pos, nodelist=cluster_nodes,
                              node_color=color, node_size=1500, alpha=0.8)

    # Draw edges
    for cluster_idx, cluster in enumerate(result.clusters):
        cluster_edges = [edge for edge in G.edges() if G.edges[edge]['cluster_id'] == cluster.cluster_id]
        color = visualizer.colors[cluster_idx % len(visualizer.colors)]

        nx.draw_networkx_edges(G, pos, edgelist=cluster_edges,
                              edge_color=color, width=2, alpha=0.6)

    # Draw labels
    labels = {node: G.nodes[node]['label'] for node in G.nodes()}
    nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold')

    plt.title("Tamil Coreference Network Visualization\n(Tamil Words mapped to Numbers)")
    plt.axis('off')
    plt.tight_layout()
    plt.show()

    # Print network statistics
    print("\nğŸ“Š NETWORK STATISTICS")
    print("=" * 30)
    print(f"Total nodes: {G.number_of_nodes()}")
    print(f"Total edges: {G.number_of_edges()}")
    print(f"Number of clusters: {len(result.clusters)}")

print("Advanced network visualization function created!")

original_example = "à®°à®¾à®®à¯ à®’à®°à¯ à®®à®¾à®£à®µà®©à¯. à®…à®µà®©à¯ à®¤à®©à®¤à¯ à®µà¯€à®Ÿà¯à®Ÿà¯ˆ à®µà®¿à®Ÿà¯à®Ÿà¯ à®•à®¿à®³à®®à¯à®ªà®¿à®©à®¾à®©à¯. à®šà¯€à®¤à®¾ à®’à®°à¯ à®†à®šà®¿à®°à®¿à®¯à¯ˆ. à®…à®µà®³à¯ à®…à®µà®©à¯ à®¤à®©à¯à®©à¯ˆ à®•à®£à¯à®Ÿ à®ªà¯‹à®¤à¯."

print(f"Analyzing: {original_example}")

# Run complete analysis
final_result = analyze_tamil_coreference(original_example)

# Show network visualization if clusters found
if final_result and final_result.clusters:
    print("\nğŸŒ Creating network visualization...")
    create_network_visualization(final_result)

    # Save results
    save_results_to_file(final_result, "demo_results.json")

#!pip install streamlit pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import json
# import re
# import plotly.graph_objects as go
# import plotly.express as px
# from plotly.subplots import make_subplots
# import pandas as pd
# import networkx as nx
# from typing import List, Dict, Tuple
# from pydantic import BaseModel
# import numpy as np
# from google import genai
# 
# # Configure Streamlit page
# st.set_page_config(
#     page_title="Tamil Coreference Resolution",
#     page_icon="ğŸ”—",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )
# 
# # Custom CSS for better styling
# st.markdown("""
# <style>
#     .main-header {
#         font-size: 2.5rem;
#         font-weight: bold;
#         text-align: center;
#         color: #1f77b4;
#         margin-bottom: 2rem;
#     }
#     .metric-card {
#         background-color: #f0f2f6;
#         padding: 1rem;
#         border-radius: 10px;
#         border-left: 5px solid #1f77b4;
#         margin: 0.5rem 0;
#     }
#     .cluster-header {
#         background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
#         color: white;
#         padding: 0.5rem 1rem;
#         border-radius: 5px;
#         margin: 0.5rem 0;
#     }
#     .word-mapping {
#         background-color: #e8f4fd;
#         padding: 1rem;
#         border-radius: 8px;
#         margin: 1rem 0;
#     }
#     .stButton > button {
#         background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
#         color: white;
#         border: none;
#         border-radius: 20px;
#         padding: 0.5rem 2rem;
#     }
# </style>
# """, unsafe_allow_html=True)
# 
# # Pydantic models
# class Mention(BaseModel):
#     text: str
#     start_index: int
#     end_index: int
#     sentence_index: int
# 
# class CoreferenceCluster(BaseModel):
#     cluster_id: int
#     entity_name: str
#     mentions: List[Mention]
#     entity_type: str
# 
# class CoreferenceResult(BaseModel):
#     text: str
#     clusters: List[CoreferenceCluster]
#     total_mentions: int
# 
# # Tamil Text Processor
# class TamilTextProcessor:
#     def __init__(self):
#         self.word_to_number = {}
#         self.number_to_word = {}
#         self.word_counter = 1
# 
#     def tokenize_tamil(self, text: str) -> List[str]:
#         return re.findall(r'\S+', text)
# 
#     def map_words_to_numbers(self, text: str) -> Dict[str, int]:
#         words = self.tokenize_tamil(text)
#         for word in words:
#             if word not in self.word_to_number:
#                 self.word_to_number[word] = self.word_counter
#                 self.number_to_word[self.word_counter] = word
#                 self.word_counter += 1
#         return self.word_to_number
# 
# # Gemini API Integration
# class GeminiCoreferenceResolver:
#     def __init__(self, api_key: str):
#         self.client = genai.Client(api_key=api_key)
# 
#     def resolve_coreferences(self, tamil_text: str) -> CoreferenceResult:
#         prompt = f"""
#         Analyze the following Tamil text and identify coreference clusters. A coreference cluster contains all mentions that refer to the same entity.
# 
#         Tamil Text: "{tamil_text}"
# 
#         Instructions:
#         1. Identify all mentions (nouns, pronouns, proper names) that refer to entities
#         2. Group mentions that refer to the same entity into clusters
#         3. For each mention, provide exact text and character positions
#         4. Classify entity types as: PERSON, PLACE, ORGANIZATION, or OTHER
#         5. Give each cluster a meaningful English entity name (e.g., "Ram", "Teacher", "Student")
# 
#         Be precise with character positions and ensure all referring expressions are captured.
#         """
# 
#         try:
#             response = self.client.models.generate_content(
#                 model="gemini-1.5-flash",
#                 contents=prompt,
#                 config={
#                     "response_mime_type": "application/json",
#                     "response_schema": CoreferenceResult,
#                 }
#             )
# 
#             result = json.loads(response.text)
#             return CoreferenceResult(**result)
# 
#         except Exception as e:
#             st.error(f"Error with Gemini API: {e}")
#             return CoreferenceResult(text=tamil_text, clusters=[], total_mentions=0)
# 
# # Initialize session state
# if 'processor' not in st.session_state:
#     st.session_state.processor = TamilTextProcessor()
# if 'resolver' not in st.session_state:
#     st.session_state.resolver = None
# if 'analysis_result' not in st.session_state:
#     st.session_state.analysis_result = None
# 
# 
# def main():
#     # Header
#     st.markdown('<h1 class="main-header">ğŸ”— Tamil Coreference Resolution System</h1>', unsafe_allow_html=True)
# 
#     # API Key input (hardcoded here, ideally should be from secrets or sidebar)
#     api_key = "AIzaSyDy9M-SIfT1vAwndfObo7xKOOvz6-Hxxxs"
# 
#     if api_key:
#         if "resolver" not in st.session_state or st.session_state.resolver is None:
#             st.session_state.resolver = GeminiCoreferenceResolver(api_key)
# 
#     st.markdown("---")
# 
#     # Sample texts
#     st.subheader("ğŸ“ Sample Texts")
#     sample_options = {
#         "Example 1": "à®°à®¾à®®à¯ à®’à®°à¯ à®®à®¾à®£à®µà®©à¯. à®…à®µà®©à¯ à®¤à®©à®¤à¯ à®µà¯€à®Ÿà¯à®Ÿà¯ˆ à®µà®¿à®Ÿà¯à®Ÿà¯ à®•à®¿à®³à®®à¯à®ªà®¿à®©à®¾à®©à¯. à®šà¯€à®¤à®¾ à®’à®°à¯ à®†à®šà®¿à®°à®¿à®¯à¯ˆ. à®…à®µà®³à¯ à®…à®µà®©à¯ à®¤à®©à¯à®©à¯ˆ à®•à®£à¯à®Ÿ à®ªà¯‹à®¤à¯.",
#         "Example 2": "à®°à®¾à®œà®¾ à®’à®°à¯ à®Ÿà®¾à®•à¯à®Ÿà®°à¯. à®…à®µà®°à¯ à®®à®°à¯à®¤à¯à®¤à¯à®µà®®à®©à¯ˆà®¯à®¿à®²à¯ à®µà¯‡à®²à¯ˆ à®šà¯†à®¯à¯à®•à®¿à®±à®¾à®°à¯. à®…à®µà®°à¯à®Ÿà¯ˆà®¯ à®®à®©à¯ˆà®µà®¿ à®’à®°à¯ à®šà¯†à®µà®¿à®²à®¿à®¯à®°à¯.",
#         "Example 3": "à®®à¯€à®©à®¾ à®ªà®³à¯à®³à®¿à®•à¯à®•à¯ à®ªà¯‹à®©à®¾à®³à¯. à®…à®µà®³à¯ à®¤à®©à¯ à®¨à®£à¯à®ªà®°à¯à®•à®³à¯ˆ à®šà®¨à¯à®¤à®¿à®¤à¯à®¤à®¾à®³à¯. à®…à®µà®°à¯à®•à®³à¯ à®’à®©à¯à®±à®¾à®• à®ªà®Ÿà®¿à®¤à¯à®¤à®©à®°à¯."
#     }
# 
#     selected_sample = st.selectbox("Choose a sample:", ["Custom"] + list(sample_options.keys()))
# 
#     # Main content area
#     col1, col2 = st.columns([2, 1])
# 
#     with col1:
#         st.subheader("ğŸ“„ Input Tamil Text")
# 
#         if selected_sample != "Custom":
#             default_text = sample_options[selected_sample]
#         else:
#             default_text = ""
# 
# 
#         tamil_text = st.text_area(
#             "Enter Tamil text for coreference analysis:",
#             value=default_text,
#             height=150,
#             help="Enter Tamil text with multiple sentences containing entities and references"
#         )
# 
#         # Analysis button
#         if st.button("ğŸ” Analyze Coreferences", disabled=not api_key or not tamil_text):
#             with st.spinner("Analyzing Tamil text ..."):
#                 # Process text
#                 word_mapping = st.session_state.processor.map_words_to_numbers(tamil_text)
# 
#                 # Resolve coreferences
#                 result = st.session_state.resolver.resolve_coreferences(tamil_text)
#                 st.session_state.analysis_result = result
# 
#                 if result.clusters:
#                     st.success(f"âœ… Analysis complete! Found {len(result.clusters)} coreference clusters.")
#                 else:
#                     st.warning("âš ï¸ No coreference clusters detected in the text.")
# 
#     with col2:
#         st.subheader("ğŸ“Š Quick Stats")
#         if st.session_state.analysis_result:
#             result = st.session_state.analysis_result
# 
#             # Metrics
#             st.metric("Total Clusters", len(result.clusters))
#             st.metric("Total Mentions", result.total_mentions)
#             st.metric("Unique Words", len(st.session_state.processor.word_to_number))
# 
#             # Entity types breakdown
#             if result.clusters:
#                 entity_types = [cluster.entity_type for cluster in result.clusters]
#                 entity_counts = pd.Series(entity_types).value_counts()
# 
#                 st.subheader("Entity Types")
#                 for entity_type, count in entity_counts.items():
#                     st.write(f"â€¢ {entity_type}: {count}")
# 
#     # Results section
#     if st.session_state.analysis_result and st.session_state.analysis_result.clusters:
#         st.markdown("---")
#         display_results(st.session_state.analysis_result)
# 
# def display_results(result: CoreferenceResult):
#     """Display analysis results with visualizations"""
# 
#     st.header("ğŸ“ˆ Analysis Results")
# 
#     # Word mapping display
#     with st.expander("ğŸ”¢ Word-to-Number Mapping", expanded=False):
#         word_mapping = st.session_state.processor.word_to_number
# 
#         # Create mapping dataframe
#         mapping_df = pd.DataFrame([
#             {"Word Number": num, "Tamil Word": word}
#             for word, num in word_mapping.items()
#         ])
# 
#         col1, col2 = st.columns(2)
#         with col1:
#             st.dataframe(mapping_df, use_container_width=True)
# 
#         with col2:
#             # Text with numbers
#             words = st.session_state.processor.tokenize_tamil(result.text)
#             number_text = " ".join([str(word_mapping.get(word, 0)) for word in words])
#             st.subheader("Text as Numbers:")
#             st.code(number_text)
# 
#     # Cluster information
#     st.subheader("ğŸ”— Detected Coreference Clusters")
# 
#     colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#98D8C8', '#F7DC6F']
# 
#     for i, cluster in enumerate(result.clusters):
#         color = colors[i % len(colors)]
# 
#         with st.container():
#             # Cluster header
#             st.markdown(f"""
#             <div class="cluster-header" style="background: {color};">
#                 <h4>Cluster {cluster.cluster_id}: {cluster.entity_name} ({cluster.entity_type})</h4>
#             </div>
#             """, unsafe_allow_html=True)
# 
#             # Mentions
#             col1, col2 = st.columns(2)
#             with col1:
#                 st.write("**Tamil Mentions:**")
#                 mentions_text = " â†’ ".join([mention.text for mention in cluster.mentions])
#                 st.write(mentions_text)
# 
#             with col2:
#                 st.write("**Number Mappings:**")
#                 mentions_numbers = " â†’ ".join([
#                     str(st.session_state.processor.word_to_number.get(mention.text, 0))
#                     for mention in cluster.mentions
#                 ])
#                 st.write(mentions_numbers)
# 
#     # Visualizations
#     st.markdown("---")
#     st.header("ğŸ“Š Visualizations")
# 
#     # Tabs for different visualizations
#     tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š Word Distribution", "ğŸŒ Arc Visualization", "ğŸ•¸ï¸ Network Graph", "ğŸ“ˆ Cluster Analysis"])
# 
#     with tab1:
#         create_word_distribution_chart(result)
# 
#     with tab2:
#         create_arc_visualization(result)
# 
#     with tab3:
#         create_network_visualization(result)
# 
#     with tab4:
#         create_cluster_analysis(result)
# 
# def create_word_distribution_chart(result: CoreferenceResult):
#     """Create word distribution bar chart"""
#     words = st.session_state.processor.tokenize_tamil(result.text)
#     word_numbers = [st.session_state.processor.word_to_number.get(word, 0) for word in words]
# 
#     fig = go.Figure()
#     fig.add_trace(go.Bar(
#         x=list(range(len(words))),
#         y=word_numbers,
#         text=[f"Word {i+1}" for i in range(len(words))],
#         textposition='auto',
#         marker_color='lightblue',
#         name='Word Numbers'
#     ))
# 
#     fig.update_layout(
#         title="Word Position vs Mapped Numbers",
#         xaxis_title="Word Position in Text",
#         yaxis_title="Assigned Number",
#         showlegend=False,
#         height=400
#     )
# 
#     st.plotly_chart(fig, use_container_width=True)
# 
# def create_arc_visualization(result: CoreferenceResult):
#     """Create arc-based coreference visualization"""
#     words = st.session_state.processor.tokenize_tamil(result.text)
#     colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']
# 
#     fig = go.Figure()
# 
#     # Add word positions
#     fig.add_trace(go.Scatter(
#         x=list(range(len(words))),
#         y=[0] * len(words),
#         mode='markers+text',
#         text=[str(st.session_state.processor.word_to_number.get(word, 0)) for word in words],
#         textposition="bottom center",
#         marker=dict(size=12, color='lightblue'),
#         name='Words (as numbers)',
#         showlegend=False
#     ))
# 
#     # Add arcs for each cluster
#     for cluster_idx, cluster in enumerate(result.clusters):
#         color = colors[cluster_idx % len(colors)]
#         mention_positions = []
# 
#         # Find positions of mentions
#         for mention in cluster.mentions:
#             for pos, word in enumerate(words):
#                 if word == mention.text:
#                     mention_positions.append(pos)
#                     break
# 
#         # Draw arcs between consecutive mentions
#         for i in range(len(mention_positions) - 1):
#             start_pos = mention_positions[i]
#             end_pos = mention_positions[i + 1]
# 
#             # Create arc
#             mid_x = (start_pos + end_pos) / 2
#             height = 0.5 + cluster_idx * 0.3
# 
#             arc_x = np.linspace(start_pos, end_pos, 50)
#             arc_y = [height * (1 - 4 * (x - mid_x) ** 2 / (end_pos - start_pos) ** 2)
#                     if end_pos != start_pos else height for x in arc_x]
# 
#             fig.add_trace(go.Scatter(
#                 x=arc_x,
#                 y=arc_y,
#                 mode='lines',
#                 line=dict(color=color, width=3),
#                 name=f'{cluster.entity_name}',
#                 showlegend=i == 0  # Only show legend for first arc of each cluster
#             ))
# 
#     fig.update_layout(
#         title="Coreference Arcs (Words shown as numbers)",
#         xaxis_title="Word Position",
#         yaxis_title="Arc Height",
#         height=500,
#         yaxis=dict(showticklabels=False)
#     )
# 
#     st.plotly_chart(fig, use_container_width=True)
# 
# def create_network_visualization(result: CoreferenceResult):
#     """Create network graph visualization"""
# 
#     # Create network data
#     nodes = []
#     edges = []
#     colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']
# 
#     for cluster_idx, cluster in enumerate(result.clusters):
#         cluster_color = colors[cluster_idx % len(colors)]
#         cluster_mentions = []
# 
#         # Add nodes for each mention
#         for i, mention in enumerate(cluster.mentions):
#             word_num = st.session_state.processor.word_to_number.get(mention.text, 0)
#             node_id = f"{cluster.entity_name}_{i}"
# 
#             nodes.append({
#                 'id': node_id,
#                 'label': f'{cluster.entity_name}\n({word_num})',
#                 'color': cluster_color,
#                 'cluster': cluster.entity_name
#             })
#             cluster_mentions.append(node_id)
# 
#         # Add edges within cluster
#         for i in range(len(cluster_mentions)):
#             for j in range(i + 1, len(cluster_mentions)):
#                 edges.append({
#                     'source': cluster_mentions[i],
#                     'target': cluster_mentions[j],
#                     'color': cluster_color
#                 })
# 
#     if not nodes:
#         st.info("No network to display - no coreference clusters found.")
#         return
# 
#     # Create network layout using networkx
#     G = nx.Graph()
#     for node in nodes:
#         G.add_node(node['id'], **node)
#     for edge in edges:
#         G.add_edge(edge['source'], edge['target'])
# 
#     pos = nx.spring_layout(G, k=2, iterations=50)
# 
#     # Create Plotly network visualization
#     fig = go.Figure()
# 
#     # Add edges
#     for edge in edges:
#         x0, y0 = pos[edge['source']]
#         x1, y1 = pos[edge['target']]
#         fig.add_trace(go.Scatter(
#             x=[x0, x1, None], y=[y0, y1, None],
#             mode='lines',
#             line=dict(width=2, color=edge['color']),
#             showlegend=False,
#             hoverinfo='none'
#         ))
# 
#     # Add nodes by cluster
#     for cluster_name in set(node['cluster'] for node in nodes):
#         cluster_nodes = [node for node in nodes if node['cluster'] == cluster_name]
#         x_coords = [pos[node['id']][0] for node in cluster_nodes]
#         y_coords = [pos[node['id']][1] for node in cluster_nodes]
#         labels = [node['label'] for node in cluster_nodes]
# 
#         fig.add_trace(go.Scatter(
#             x=x_coords, y=y_coords,
#             mode='markers+text',
#             marker=dict(size=20, color=cluster_nodes[0]['color']),
#             text=labels,
#             textposition="middle center",
#             name=cluster_name,
#             textfont=dict(size=10, color='white')
#         ))
# 
#     fig.update_layout(
#         title="Coreference Network Graph",
#         showlegend=True,
#         height=600,
#         xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
#         yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
#     )
# 
#     st.plotly_chart(fig, use_container_width=True)
# 
# def create_cluster_analysis(result: CoreferenceResult):
#     """Create cluster analysis charts"""
# 
#     col1, col2 = st.columns(2)
# 
#     with col1:
#         # Cluster size distribution
#         cluster_sizes = [len(cluster.mentions) for cluster in result.clusters]
#         cluster_names = [cluster.entity_name for cluster in result.clusters]
# 
#         fig1 = px.bar(
#             x=cluster_names,
#             y=cluster_sizes,
#             title="Mentions per Cluster",
#             labels={'x': 'Entity', 'y': 'Number of Mentions'},
#             color=cluster_sizes,
#             color_continuous_scale='viridis'
#         )
#         fig1.update_layout(height=400)
#         st.plotly_chart(fig1, use_container_width=True)
# 
#     with col2:
#         # Entity type distribution
#         entity_types = [cluster.entity_type for cluster in result.clusters]
#         entity_type_counts = pd.Series(entity_types).value_counts()
# 
#         fig2 = px.pie(
#             values=entity_type_counts.values,
#             names=entity_type_counts.index,
#             title="Entity Type Distribution"
#         )
#         fig2.update_layout(height=400)
#         st.plotly_chart(fig2, use_container_width=True)
# 
#     # Detailed cluster information table
#     st.subheader("ğŸ“‹ Detailed Cluster Information")
# 
#     cluster_data = []
#     for cluster in result.clusters:
#         cluster_data.append({
#             'Cluster ID': cluster.cluster_id,
#             'Entity Name': cluster.entity_name,
#             'Entity Type': cluster.entity_type,
#             'Mention Count': len(cluster.mentions),
#             'Tamil Mentions': ' | '.join([mention.text for mention in cluster.mentions]),
#             'Number Mappings': ' | '.join([
#                 str(st.session_state.processor.word_to_number.get(mention.text, 0))
#                 for mention in cluster.mentions
#             ])
#         })
# 
#     cluster_df = pd.DataFrame(cluster_data)
#     st.dataframe(cluster_df, use_container_width=True)
# 
#     # Download results
#     if st.button("ğŸ’¾ Download Results as JSON"):
#         results_json = result.dict()
#         st.download_button(
#             label="Download JSON",
#             data=json.dumps(results_json, indent=2, ensure_ascii=False),
#             file_name="tamil_coreference_results.json",
#             mime="application/json"
#         )
# 
if __name__ == "__main__":
   main()

